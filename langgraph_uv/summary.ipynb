{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afcb5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import fitz  # pymupdf\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828a827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fa9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹ ===\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    \"\"\"PyMuPDFë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - í•œê¸€ ì²˜ë¦¬ ê°œì„ \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        print(f\"PDF íŒŒì¼: {pdf_path}\")\n",
    "        print(f\"ì´ í˜ì´ì§€ ìˆ˜: {len(doc)}\")\n",
    "        \n",
    "        full_text = \"\"\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            print(f\"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì¤‘...\")\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            text = page.get_text()\n",
    "            \n",
    "            if text and text.strip():\n",
    "                # í•œê¸€ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì •ë¦¬\n",
    "                text = text.replace('\\x00', '')  # null ë¬¸ì ì œê±°\n",
    "                text = text.replace('\\ufeff', '')  # BOM ì œê±°\n",
    "                text = text.replace('\\r\\n', '\\n')  # ì¤„ë°”ê¿ˆ ì •ë¦¬\n",
    "                text = text.replace('\\r', '\\n')\n",
    "                \n",
    "                print(f\"  í˜ì´ì§€ {page_num + 1}: {len(text)} ê¸€ì ì¶”ì¶œ\")\n",
    "                print(f\"  ì²« 50ê¸€ì: {text[:50]}\")\n",
    "                \n",
    "                full_text += text + \"\\n\\n\"  # í˜ì´ì§€ êµ¬ë¶„\n",
    "            else:\n",
    "                print(f\"  í˜ì´ì§€ {page_num + 1}: í…ìŠ¤íŠ¸ ì—†ìŒ\")\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"\\nì´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {len(full_text)} ê¸€ì\")\n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF ì˜¤ë¥˜: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=2000, overlap=100):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    if not text.strip():\n",
    "        print(\"ë¹ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        if chunk:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9da585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ í•¨ìˆ˜ë“¤ ===\n",
    "def search_documents_openai(query, index, embedding_model, chunks, metadatas, k=3):\n",
    "    \"\"\"OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
    "    try:\n",
    "        # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜\n",
    "        query_embedding = embedding_model.embed_query(query)\n",
    "        query_embedding = np.array([query_embedding]).astype('float32')\n",
    "        \n",
    "        # FAISSë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        \n",
    "        # ê²°ê³¼ í¬ë§·íŒ…\n",
    "        results = []\n",
    "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx < len(chunks):  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸\n",
    "                results.append({\n",
    "                    'chunk_id': idx,\n",
    "                    'content': chunks[idx],\n",
    "                    'score': 1.0 - (distance / 2.0),  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜\n",
    "                    'metadata': metadatas[idx] if idx < len(metadatas) else {}\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_vectorstore_openai(index, chunks, metadatas, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\"\"\"\n",
    "    try:\n",
    "        # FAISS ì¸ë±ìŠ¤ ì €ì¥\n",
    "        faiss.write_index(index, f'{save_name}_vectors.index')\n",
    "        \n",
    "        # ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "        with open(f'{save_name}_data.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'chunks': chunks,\n",
    "                'metadatas': metadatas,\n",
    "                'pdf_filename': pdf_filename,\n",
    "                'chunk_count': len(chunks),\n",
    "                'embedding_model': 'text-embedding-3-large',\n",
    "                'embedding_type': 'openai'\n",
    "            }, f)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_faiss_vectorstore_openai(chunks, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"ì²­í¬ë“¤ì„ FAISS ë²¡í„° ìŠ¤í† ì–´ë¡œ ë³€í™˜í•˜ê³  ìë™ ì €ì¥ (OpenAI ì„ë² ë”© ì‚¬ìš©)\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(\"OpenAI ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "    \n",
    "    print(f\"ì„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)}ê°œ ì²­í¬)\")\n",
    "    print(\"OpenAI API í˜¸ì¶œ ì¤‘ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...\")\n",
    "    \n",
    "    try:\n",
    "        # ì²­í¬ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜\n",
    "        embeddings = embedding.embed_documents(chunks)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        print(f\"ì„ë² ë”© ì™„ë£Œ! ì°¨ì›: {embeddings.shape[1]}, ê°œìˆ˜: {embeddings.shape[0]}\")\n",
    "        \n",
    "        # FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)  # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "        metadatas = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadatas.append({\n",
    "                'chunk_id': i,\n",
    "                'source': pdf_filename,\n",
    "                'chunk_size': len(chunk),\n",
    "                'preview': chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "            })\n",
    "        \n",
    "        print(f\"FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! {index.ntotal}ê°œ ë²¡í„° ì €ì¥ë¨\")\n",
    "        \n",
    "        # ìë™ ì €ì¥\n",
    "        print(\"ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìë™ ì €ì¥ ì¤‘...\")\n",
    "        if save_vectorstore_openai(index, chunks, metadatas, pdf_filename, save_name):\n",
    "            print(\"âœ“ ìë™ ì €ì¥ ì™„ë£Œ!\")\n",
    "        else:\n",
    "            print(\"âš ï¸  ìë™ ì €ì¥ ì‹¤íŒ¨\")\n",
    "        \n",
    "        return index, embedding, chunks, metadatas\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "        print(\"OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_or_create_vectorstore(chunks, pdf_filename, save_name=\"pdf_openai_vectors\"):\n",
    "    \"\"\"ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    index_file = f'{save_name}_vectors.index'\n",
    "    data_file = f'{save_name}_data.pkl'\n",
    "    \n",
    "    # ì €ì¥ëœ íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "    if os.path.exists(index_file) and os.path.exists(data_file):\n",
    "        print(\"ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë¡œë“œ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            # ì €ì¥ëœ ë°ì´í„° ë¡œë“œ\n",
    "            index = faiss.read_index(index_file)\n",
    "            \n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # ì €ì¥ëœ PDF íŒŒì¼ëª…ê³¼ í˜„ì¬ íŒŒì¼ëª… ë¹„êµ\n",
    "            if data['pdf_filename'] == pdf_filename:\n",
    "                print(\"âœ“ ë™ì¼í•œ PDF íŒŒì¼ì˜ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì„±ê³µ!\")\n",
    "                \n",
    "                # OpenAI ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "                embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "                \n",
    "                print(f\"- ë²¡í„° ê°œìˆ˜: {index.ntotal}\")\n",
    "                print(f\"- ì²­í¬ ê°œìˆ˜: {data['chunk_count']}\")\n",
    "                print(f\"- ì›ë³¸ íŒŒì¼: {data['pdf_filename']}\")\n",
    "                print(f\"- ì„ë² ë”© ëª¨ë¸: {data.get('embedding_model', 'ì •ë³´ ì—†ìŒ')}\")\n",
    "                \n",
    "                return index, embedding_model, data['chunks'], data['metadatas']\n",
    "            else:\n",
    "                print(f\"âš ï¸  ë‹¤ë¥¸ PDF íŒŒì¼ì˜ ë²¡í„° ìŠ¤í† ì–´ì…ë‹ˆë‹¤.\")\n",
    "                print(f\"   ì €ì¥ëœ íŒŒì¼: {data['pdf_filename']}\")\n",
    "                print(f\"   í˜„ì¬ íŒŒì¼: {pdf_filename}\")\n",
    "                print(\"   ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"   ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "    print(\"\\n=== ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ===\")\n",
    "    return create_faiss_vectorstore_openai(chunks, pdf_filename, save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60106820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ìƒíƒœ ì •ì˜ ===\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Any]\n",
    "    user_query: str\n",
    "    search_results: List[Dict]\n",
    "    summary: str\n",
    "    tool_calls: List[Dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2cad229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì „ì—­ ë³€ìˆ˜ë“¤ ===\n",
    "# ì „ì—­ ë³€ìˆ˜ë¡œ ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ ê°ì²´ë“¤ì„ ì €ì¥\n",
    "vector_store_data = {\n",
    "    'index': None,\n",
    "    'embedding_model': None,\n",
    "    'chunks': None,\n",
    "    'metadatas': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e681d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë„êµ¬ ì •ì˜ ===\n",
    "@tool\n",
    "def search_pdf_documents(query: str, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        query: ê²€ìƒ‰í•  í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸\n",
    "        k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)\n",
    "    \n",
    "    Returns:\n",
    "        ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ë“¤\n",
    "    \"\"\"\n",
    "    if vector_store_data['index'] is None:\n",
    "        return \"ë²¡í„° ìŠ¤í† ì–´ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\"\n",
    "    \n",
    "    try:\n",
    "        # ê²€ìƒ‰ ì‹¤í–‰\n",
    "        results = search_documents_openai(\n",
    "            query, \n",
    "            vector_store_data['index'], \n",
    "            vector_store_data['embedding_model'], \n",
    "            vector_store_data['chunks'], \n",
    "            vector_store_data['metadatas'], \n",
    "            k\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            return f\"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            formatted_results.append(\n",
    "                f\"[ê²€ìƒ‰ê²°ê³¼ {i+1}]\\n\"\n",
    "                f\"ìœ ì‚¬ë„: {result['score']:.4f}\\n\"\n",
    "                f\"ë‚´ìš©: {result['content'][:500]}{'...' if len(result['content']) > 500 else ''}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def summarize_content(content: str, focus: str = \"general\") -> str:\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        content: ìš”ì•½í•  ë‚´ìš©\n",
    "        focus: ìš”ì•½ ì´ˆì  (\"general\", \"key_points\", \"technical\", \"brief\")\n",
    "    \n",
    "    Returns:\n",
    "        ìš”ì•½ëœ ë‚´ìš©\n",
    "    \"\"\"\n",
    "    if not content.strip():\n",
    "        return \"ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    # OpenAI LLM ëª¨ë¸ ì‚¬ìš©\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "    \n",
    "    focus_prompts = {\n",
    "        \"general\": \"ë‹¤ìŒ ë‚´ìš©ì„ ì „ë°˜ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\",\n",
    "        \"key_points\": \"ë‹¤ìŒ ë‚´ìš©ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ë“¤ë§Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:\",\n",
    "        \"technical\": \"ë‹¤ìŒ ë‚´ìš©ì—ì„œ ê¸°ìˆ ì ì¸ ë¶€ë¶„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\",\n",
    "        \"brief\": \"ë‹¤ìŒ ë‚´ìš©ì„ ê°„ë‹¨íˆ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\"\n",
    "    }\n",
    "    \n",
    "    prompt = focus_prompts.get(focus, focus_prompts[\"general\"])\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([\n",
    "            HumanMessage(content=f\"{prompt}\\n\\n{content}\")\n",
    "        ])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_document_info() -> str:\n",
    "    \"\"\"\n",
    "    í˜„ì¬ ë¡œë“œëœ PDF ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Returns:\n",
    "        ë¬¸ì„œ ì •ë³´\n",
    "    \"\"\"\n",
    "    if vector_store_data['chunks'] is None:\n",
    "        return \"ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    total_chars = sum(len(chunk) for chunk in vector_store_data['chunks'])\n",
    "    \n",
    "    return f\"\"\"\n",
    "í˜„ì¬ ë¡œë“œëœ ë¬¸ì„œ ì •ë³´:\n",
    "- íŒŒì¼ëª…: {vector_store_data.get('pdf_filename', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n",
    "- ì´ ì²­í¬ ìˆ˜: {len(vector_store_data['chunks'])}\n",
    "- ì´ ê¸€ì ìˆ˜: {total_chars:,}\n",
    "- í‰ê·  ì²­í¬ í¬ê¸°: {total_chars // len(vector_store_data['chunks']):,} ê¸€ì\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87bc3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì—ì´ì „íŠ¸ ë…¸ë“œ í•¨ìˆ˜ë“¤ ===\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ ë„êµ¬ ì‹¤í–‰\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ\n",
    "    return END\n",
    "\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"LLM ëª¨ë¸ í˜¸ì¶œ\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "    system_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ê²€ìƒ‰ ë° ìš”ì•½ ì „ë¬¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:\n",
    "1. search_pdf_documents: PDFì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰\n",
    "2. summarize_content: ë‚´ìš© ìš”ì•½\n",
    "3. get_document_info: ë¬¸ì„œ ì •ë³´ ì¡°íšŒ\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:\n",
    "- íŠ¹ì • ë‚´ìš©ì„ ì°¾ê³  ì‹¶ë‹¤ë©´ search_pdf_documentsë¥¼ ì‚¬ìš©\n",
    "- ê²€ìƒ‰ëœ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì‹¶ë‹¤ë©´ summarize_contentë¥¼ ì‚¬ìš©\n",
    "- ë¬¸ì„œ ì •ë³´ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ get_document_infoë¥¼ ì‚¬ìš©\n",
    "\n",
    "í•­ìƒ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—†ë‹¤ë©´ ì¶”ê°€\n",
    "    if not messages or not any(hasattr(msg, 'content') and system_prompt in str(msg.content) for msg in messages[:1]):\n",
    "        messages = [HumanMessage(content=system_prompt)] + messages\n",
    "    \n",
    "    # LLM í˜¸ì¶œ\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)\n",
    "    llm_with_tools = llm.bind_tools([search_pdf_documents, summarize_content, get_document_info])\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": messages + [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f37272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ ë…¸ë“œ ìƒì„±\n",
    "tools = [search_pdf_documents, summarize_content, get_document_info]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def call_tools(state: AgentState) -> AgentState:\n",
    "    \"\"\"ë„êµ¬ ì‹¤í–‰ - ToolNodeë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ë°©ì‹\"\"\"\n",
    "    return tool_node.invoke(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e37b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì—ì´ì „íŠ¸ ìƒì„± ===\n",
    "def create_pdf_agent():\n",
    "    \"\"\"PDF ê²€ìƒ‰ ìš”ì•½ ì—ì´ì „íŠ¸ ìƒì„±\"\"\"\n",
    "    \n",
    "    # ê·¸ë˜í”„ ìƒì„±\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "\n",
    "    # ë…¸ë“œ ì¶”ê°€\n",
    "    graph_builder.add_node(\"agent\", call_model)\n",
    "    graph_builder.add_node(\"tools\", call_tools)\n",
    "\n",
    "    # ì‹œì‘ì  ì„¤ì •\n",
    "    graph_builder.set_entry_point(\"agent\")\n",
    "\n",
    "    # ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ë„êµ¬ì—ì„œ ë‹¤ì‹œ ì—ì´ì „íŠ¸ë¡œ\n",
    "    graph_builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    # ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "    graph = graph_builder.compile()\n",
    "\n",
    "    return graph\n",
    "\n",
    "# === ì‹¤í–‰ í•¨ìˆ˜ë“¤ ===\n",
    "def run_pdf_agent(user_query: str, pdf_agent) -> str:\n",
    "    \"\"\"PDF ì—ì´ì „íŠ¸ ì‹¤í–‰\"\"\"\n",
    "    \n",
    "    # ì´ˆê¸° ìƒíƒœ\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"user_query\": user_query,\n",
    "        \"search_results\": [],\n",
    "        \"summary\": \"\",\n",
    "        \"tool_calls\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # ì—ì´ì „íŠ¸ ì‹¤í–‰\n",
    "        result = pdf_agent.invoke(initial_state)\n",
    "        \n",
    "        # ìµœì¢… ì‘ë‹µ ì¶”ì¶œ\n",
    "        final_message = result[\"messages\"][-1]\n",
    "        if hasattr(final_message, 'content'):\n",
    "            return final_message.content\n",
    "        else:\n",
    "            return str(final_message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a5a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_pdf_agent(pdf_agent):\n",
    "    \"\"\"ëŒ€í™”í˜• PDF ì—ì´ì „íŠ¸\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¤– PDF ê²€ìƒ‰ ìš”ì•½ ì—ì´ì „íŠ¸\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"â€¢ 'ì¢…ë£Œ', 'quit', 'exit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤\")\n",
    "    print(\"â€¢ ì˜ˆì‹œ ì§ˆë¬¸:\")\n",
    "    print(\"  - 'êµí†µì•½ìì— ëŒ€í•´ ê²€ìƒ‰í•´ì„œ ìš”ì•½í•´ì¤˜'\")\n",
    "    print(\"  - 'ê³ ë ¹ì ì •ì±… ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ì¤˜'\")\n",
    "    print(\"  - 'ë¬¸ì„œ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜'\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['ì¢…ë£Œ', 'quit', 'exit', 'q']:\n",
    "                print(\"ì—ì´ì „íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸ‘‹\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nğŸ¤– ì—ì´ì „íŠ¸ê°€ ì‘ì—… ì¤‘ì…ë‹ˆë‹¤...\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # ì—ì´ì „íŠ¸ ì‹¤í–‰\n",
    "            response = run_pdf_agent(user_input, pdf_agent)\n",
    "            \n",
    "            print(f\"\\nğŸ¯ ë‹µë³€:\\n{response}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nì—ì´ì „íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ğŸ‘‹\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150f8747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ===\n",
      "PDF íŒŒì¼: 2024-PR-15.pdf\n",
      "ì´ í˜ì´ì§€ ìˆ˜: 127\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m         interactive_pdf_agent(pdf_agent)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_with_fitz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m full_text:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ— í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mextract_text_with_fitz\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m page \u001b[38;5;241m=\u001b[39m doc[page_num]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# í…ìŠ¤íŠ¸ ì¶”ì¶œ\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mand\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# í•œê¸€ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì •ë¦¬\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# null ë¬¸ì ì œê±°\u001b[39;00m\n",
      "File \u001b[1;32mc:\\AI_Prompt\\workspace\\ai_agent_work2\\langgraph_uv\\.venv\\Lib\\site-packages\\pymupdf\\utils.py:1002\u001b[0m, in \u001b[0;36mget_text\u001b[1;34m(page, option, clip, flags, textpage, sort, delimiters, tolerance)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     t \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mextractXHTML()\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1002\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractText\u001b[49m\u001b[43m(\u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m textpage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m tp\n",
      "File \u001b[1;32mc:\\AI_Prompt\\workspace\\ai_agent_work2\\langgraph_uv\\.venv\\Lib\\site-packages\\pymupdf\\__init__.py:12267\u001b[0m, in \u001b[0;36mTextPage.extractText\u001b[1;34m(self, sort)\u001b[0m\n\u001b[0;32m  12265\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return simple, bare text on the page.\"\"\"\u001b[39;00m\n\u001b[0;32m  12266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort:\n\u001b[1;32m> 12267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extractText\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m  12268\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractBLOCKS()[:]\n\u001b[0;32m  12269\u001b[0m blocks\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m b: (b[\u001b[38;5;241m3\u001b[39m], b[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\AI_Prompt\\workspace\\ai_agent_work2\\langgraph_uv\\.venv\\Lib\\site-packages\\pymupdf\\__init__.py:12073\u001b[0m, in \u001b[0;36mTextPage._extractText\u001b[1;34m(self, format_)\u001b[0m\n\u001b[0;32m  12071\u001b[0m     JM_print_stext_page_as_text(res, this_tpage)\n\u001b[0;32m  12072\u001b[0m out\u001b[38;5;241m.\u001b[39mfz_close_output()\n\u001b[1;32m> 12073\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mJM_EscapeStrFromBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  12074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mc:\\AI_Prompt\\workspace\\ai_agent_work2\\langgraph_uv\\.venv\\Lib\\site-packages\\pymupdf\\__init__.py:14701\u001b[0m, in \u001b[0;36mJM_EscapeStrFromBuffer\u001b[1;34m(buff)\u001b[0m\n\u001b[0;32m  14699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m  14700\u001b[0m s \u001b[38;5;241m=\u001b[39m mupdf\u001b[38;5;241m.\u001b[39mfz_buffer_extract_copy(buff)\n\u001b[1;32m> 14701\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mPyUnicode_DecodeRawUnicodeEscape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m  14702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[1;32mc:\\AI_Prompt\\workspace\\ai_agent_work2\\langgraph_uv\\.venv\\Lib\\site-packages\\pymupdf\\__init__.py:17448\u001b[0m, in \u001b[0;36mPyUnicode_DecodeRawUnicodeEscape\u001b[1;34m(s, errors)\u001b[0m\n\u001b[0;32m  17446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m  17447\u001b[0m     rc \u001b[38;5;241m=\u001b[39m s[:]\n\u001b[1;32m> 17448\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_unicode_escape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  17449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\__init__.py:99\u001b[0m, in \u001b[0;36msearch_function\u001b[1;34m(encoding)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Import is absolute to prevent the possibly malicious import of a\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# module with side-effects that is not in the 'encodings' package.\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencodings.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m modname, fromlist\u001b[38;5;241m=\u001b[39m_import_tail,\n\u001b[0;32m    100\u001b[0m                      level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# ImportError may occur because 'encodings.(modname)' does not exist,\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# or because it imports a name that does not exist (see mbcs and oem)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1507\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1479\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1615\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === ë©”ì¸ ì‹¤í–‰ ===\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    pdf_file = \"2024-PR-15.pdf\"\n",
    "    \n",
    "    if not os.path.exists(pdf_file):\n",
    "        print(f\"âœ— PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ===\")\n",
    "    full_text = extract_text_with_fitz(pdf_file)\n",
    "    \n",
    "    if not full_text:\n",
    "        print(\"âœ— í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return\n",
    "    \n",
    "    print(\"âœ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ!\")\n",
    "    \n",
    "    # 2. í…ìŠ¤íŠ¸ ì²­í‚¹\n",
    "    print(\"\\n=== í…ìŠ¤íŠ¸ ì²­í‚¹ ===\")\n",
    "    chunks = chunk_text(full_text)\n",
    "    print(f\"âœ“ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬\")\n",
    "    \n",
    "    # 3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±/ë¡œë“œ\n",
    "    print(\"\\n=== ë²¡í„° ìŠ¤í† ì–´ ì²˜ë¦¬ ===\")\n",
    "    index, embedding_model, chunks, metadatas = load_or_create_vectorstore(\n",
    "        chunks, \n",
    "        pdf_file, \n",
    "        save_name=\"my_pdf_vectors\"\n",
    "    )\n",
    "    \n",
    "    if index is None:\n",
    "        print(\"âœ— ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì‹¤íŒ¨\")\n",
    "        return\n",
    "    \n",
    "    print(\"âœ“ ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    \n",
    "    # 4. ì „ì—­ ë³€ìˆ˜ì— ì €ì¥\n",
    "    vector_store_data['index'] = index\n",
    "    vector_store_data['embedding_model'] = embedding_model\n",
    "    vector_store_data['chunks'] = chunks\n",
    "    vector_store_data['metadatas'] = metadatas\n",
    "    vector_store_data['pdf_filename'] = pdf_file\n",
    "    \n",
    "    # 5. ì—ì´ì „íŠ¸ ìƒì„±\n",
    "    print(\"\\n=== PDF ì—ì´ì „íŠ¸ ìƒì„± ===\")\n",
    "    pdf_agent = create_pdf_agent()\n",
    "    print(\"âœ“ PDF ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ!\")\n",
    "    \n",
    "    # 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "    test_query = \"ë¬¸ì„œ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\"\n",
    "    print(f\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_query}\")\n",
    "    result = run_pdf_agent(test_query, pdf_agent)\n",
    "    print(f\"ê²°ê³¼: {result}\")\n",
    "    \n",
    "    # 7. ëŒ€í™”í˜• ì‹¤í–‰\n",
    "    print(\"\\nğŸš€ ëŒ€í™”í˜• ì—ì´ì „íŠ¸ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)\")\n",
    "    if input(\"ì…ë ¥: \").lower() in ['y', 'yes', 'ë„¤', 'ã…‡']:\n",
    "        interactive_pdf_agent(pdf_agent)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
